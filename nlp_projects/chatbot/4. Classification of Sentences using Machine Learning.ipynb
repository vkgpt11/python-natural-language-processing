{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/edbullen/nltk/blob/master/classification.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "CODE_LOC = 'D:\\\\Git\\\\python-natural-language-processing\\\\chatbot\\\\'   # !! Modify to path to \"features.py\" folder lcoation\n",
    "DATA_LOC = 'D:\\\\Git\\\\python-natural-language-processing\\\\chatbot\\\\sentences.csv'  # !! Modify this to the CSV data location\n",
    "\n",
    "sentences = pd.read_csv(filepath_or_buffer = DATA_LOC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SENTENCE</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sorry, I don't know about the weather.</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>That is a tricky question to answer.</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What does OCM stand for</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MAX is a Mobile Application Accelerator</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Can a dog see in colour?</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>how are you</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>If you deploy a MySQL database in the Oracle c...</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>who is dominic Fakename</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>what's the weather like today?</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Can the OCM host non Oracle software stacks?</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            SENTENCE CLASS\n",
       "0             Sorry, I don't know about the weather.     S\n",
       "1               That is a tricky question to answer.     C\n",
       "2                            What does OCM stand for     Q\n",
       "3            MAX is a Mobile Application Accelerator     S\n",
       "4                           Can a dog see in colour?     Q\n",
       "5                                        how are you     C\n",
       "6  If you deploy a MySQL database in the Oracle c...     Q\n",
       "7                            who is dominic Fakename     Q\n",
       "8                     what's the weather like today?     C\n",
       "9       Can the OCM host non Oracle software stacks?     Q"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Feature Engineering - A Non-Standard, Bespoke Approch\n",
    "\n",
    "Chapter 6 of the NLTK Book has a great deal of background and worked examples for classifying text using machine learning algorithms such as Naive Bayes Classifiers. A different bespoke approach involving home-grown feature engineering and a scikit-learn Random Forest model is outlined in this note.\n",
    "\n",
    "The code snippet below is an example of taking a sentence and extracting sets of POS-tag Triples from it. We can use this approach for building up features from a sentence by counting occurances of triple-patterns (or other POS-tag patterns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words mapped to Part of Speech Tags: [('Can', 'MD'), ('a', 'DT'), ('dog', 'NN'), ('see', 'NN'), ('in', 'IN'), ('Colour', 'NNP'), ('?', '.')]\n",
      "PoS Tags: ['MD', 'DT', 'NN', 'NN', 'IN', 'NNP', '.']\n",
      "Sequence of triples: ['MD-DT-NN', 'DT-NN-NN', 'NN-NN-IN', 'NN-IN-NNP']\n"
     ]
    }
   ],
   "source": [
    "# Extract some patterns of PoS sequences \n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "\n",
    "list_of_triple_strings = [] #triple sequence of PoS tags\n",
    "sentence = \"Can a dog see in Colour?\"\n",
    "\n",
    "sentence_parsed = word_tokenize(sentence)\n",
    "#print(sentence_parsed)\n",
    "pos_tags = nltk.pos_tag(sentence_parsed)\n",
    "#print(pos_tags)\n",
    "pos = [ i[1] for i in pos_tags ]\n",
    "print(\"Words mapped to Part of Speech Tags:\",pos_tags)\n",
    "print(\"PoS Tags:\", pos)\n",
    "\n",
    "n = len(pos)\n",
    "for i in range(0,n-3):\n",
    "    t = \"-\".join(pos[i:i+3]) # pull out 3 list item from counter, convert to string\n",
    "    list_of_triple_strings.append(t)\n",
    "    \n",
    "print(\"Sequence of triples:\", list_of_triple_strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Features\n",
    "\n",
    "After pre-processing the sentences (using the approach above) we can get a set of triples for Questions, Chat, Statements. There will be a lot of intersection, but hopefully some clear patterns\n",
    "## The features.py Features Generator\n",
    "This is a custom Python module to extract features from a sentence, written for this ChatBot demo.\n",
    "\n",
    "features.py is located here: https://github.com/edbullen/NLPBot/releases/download/SupportingFiles/features.py\n",
    "\n",
    "Just\n",
    "\n",
    "`import features`\n",
    "\n",
    "and call\n",
    "\n",
    "`features = features_dict(id,sentence, c)`\n",
    "\n",
    "to extract a dictionary of features for the given sentence.\n",
    "\n",
    "* The \"id\" can be any arbirtary ID value - it just get s passed in and passout as an ID identifier in the resultant dictionary.\n",
    "* The \"c\" value can also be any arbitrary value representing the Class label - the idea is to supply an appropriate label so that the dict that is passed back has all the necessary information in it.\n",
    "\n",
    "The actual features that are generated and the logic behind how this is done is all hard-coded in features.py (it is not paramaterised - a potential enhancement that could be added)\n",
    "\n",
    "#### features.py POS Triples Extract\n",
    "\n",
    "The features.py module includes a function\n",
    "\n",
    "`get_triples(pos)`\n",
    "\n",
    "which returns a string of the form \"POS-POS-POS\" where \"POS\" is a Part-Of-Speech tag.\n",
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can a dog see in colour\n",
      "['MD-DT-NN', 'DT-NN-NN', 'NN-NN-IN', 'NN-IN-NN']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(CODE_LOC)\n",
    "import features\n",
    "\n",
    "sentence = \"Can a dog see in colour?\"\n",
    "\n",
    "sentence = features.strip_sentence(sentence)\n",
    "print(sentence)\n",
    "\n",
    "pos = features.get_pos(sentence)\n",
    "triples = features.get_triples(pos)\n",
    "\n",
    "print(triples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'verbBeforeNoun': 1, 'stemmedCount': 4, 'CD': 0, 'qVerbCombo': 1, 'VBG': 0, 'endTuple0': 1, 'NNP': 0, 'NNS': 0, 'id': '1', 'startTuple0': 0, 'qTripleScore': 0, 'NN': 3, 'NNPS': 0, 'qMark': 1, 'PRP': 0, 'VBZ': 0, 'stemmedEndNN': 0, 'class': 'X', 'wordCount': 6, 'sTripleScore': 0, 'endTuple1': 0, 'endTuple2': 0}\n",
      "1,6,4,1,1,1,0,0,0,3,0,0,0,0,0,0,1,0,0,0,0,X \n",
      "\n",
      "{'verbBeforeNoun': 0, 'stemmedCount': 3, 'CD': 0, 'qVerbCombo': 1, 'VBG': 1, 'endTuple0': 0, 'NNP': 2, 'NNS': 0, 'id': '2', 'startTuple0': 0, 'qTripleScore': 0, 'NN': 0, 'NNPS': 0, 'qMark': 1, 'PRP': 1, 'VBZ': 0, 'stemmedEndNN': 0, 'class': 'X', 'wordCount': 4, 'sTripleScore': 0, 'endTuple1': 0, 'endTuple2': 0}\n",
      "2,4,3,1,1,0,1,0,2,0,0,0,1,0,0,0,0,0,0,0,0,X \n",
      "\n",
      "{'verbBeforeNoun': 0, 'stemmedCount': 8, 'CD': 3, 'qVerbCombo': 1, 'VBG': 0, 'endTuple0': 0, 'NNP': 1, 'NNS': 1, 'id': '3', 'startTuple0': 0, 'qTripleScore': 0, 'NN': 1, 'NNPS': 0, 'qMark': 0, 'PRP': 0, 'VBZ': 0, 'stemmedEndNN': 0, 'class': 'X', 'wordCount': 12, 'sTripleScore': 2, 'endTuple1': 0, 'endTuple2': 0}\n",
      "3,12,8,1,0,0,0,0,1,1,1,0,0,3,0,0,0,0,0,0,2,X \n",
      "\n",
      "{'verbBeforeNoun': 1, 'stemmedCount': 4, 'CD': 1, 'qVerbCombo': 1, 'VBG': 0, 'endTuple0': 0, 'NNP': 0, 'NNS': 0, 'id': '4', 'startTuple0': 0, 'qTripleScore': 4, 'NN': 0, 'NNPS': 0, 'qMark': 0, 'PRP': 0, 'VBZ': 0, 'stemmedEndNN': 0, 'class': 'X', 'wordCount': 6, 'sTripleScore': 0, 'endTuple1': 1, 'endTuple2': 0}\n",
      "4,6,4,1,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,4,0,X \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### Bespoke Features Generator Example - Get Python dictonary of feature ####\n",
    "\n",
    "sentences = [\"Can a dog see in colour?\",\n",
    "             \"Hey, How's it going?\",\n",
    "             \"Oracle 12.2 will be released for on-premises users on 15 March 2017\",\n",
    "             \"When will Oracle 12 be released\"]\n",
    "\n",
    "id = 1\n",
    "for s in sentences:\n",
    "    features_dict = features.features_dict(str(id),s)\n",
    "    features_string, header = features.get_string(str(id),s)\n",
    "    print(features_dict)\n",
    "    print(features_string, \"\\n\")\n",
    "    id += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this approach we can bulk generate some numeric data-features generated from a CSV file of sentences. If each sentence has a unique ID and we have a classifier label (S/Q/C) for each row observation, we can now try to build a ML classification model and assess it's effectiveness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Build a Machine Learning Model\n",
    "In this section we load a features CSV file called featuresDump.csv into a Pandas data-frame. The data was generated with features.py reading in the sentences.csv file as described in the previous section. The featuresDump.csv data is then used to train a Random Forest model to predict whether a sentence is Chat, Statement or Question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 rows loaded\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "FNAME = 'featuresDump.csv'\n",
    "\n",
    "df = pd.read_csv(filepath_or_buffer=FNAME,)\n",
    "print(len(df),\"rows loaded\")\n",
    "\n",
    "# strip any leading spaces from col names\n",
    "df.columns = df.columns[:].str.strip()\n",
    "df['class'] = df['class'].map(lambda x:x.strip())\n",
    "\n",
    "width = df.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and testing split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77  rows split into training set, 23 split into test set.\n",
      "FEATURES = Index(['wordCount', 'stemmedCount', 'stemmedEndNN', 'CD', 'NN', 'NNP', 'NNPS',\n",
      "       'NNS', 'PRP', 'VBG', 'VBZ', 'startTuple0', 'endTuple0', 'endTuple1',\n",
      "       'endTuple2', 'verbBeforeNoun', 'qMark', 'qVerbCombo', 'qTripleScore',\n",
      "       'sTripleScore'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#split into test and training (is_train: True / False col)\n",
    "np.random.seed(seed=1)\n",
    "df['is_train'] = np.random.uniform(0, 1, len(df)) <= .75\n",
    "train, test = df[df['is_train']==True], df[df['is_train']==False]\n",
    "print(str(len(train)), \" rows split into training set,\", str(len(test)), \"split into test set.\")\n",
    "\n",
    "features = df.columns[1:width-1]  #remove the first ID col and last col=classifier\n",
    "print(\"FEATURES = {}\".format(features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a Model with the Training Data-Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=100, n_jobs=2, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit an RF Model for \"class\" given features\n",
    "clf = RandomForestClassifier(n_jobs=2, n_estimators = 100)\n",
    "clf.fit(train[features], train['class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Predictions from the Test Data-Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds = clf.predict(test[features])\n",
    "predout = pd.DataFrame({ 'id' : test['id'], 'predicted' : preds, 'actual' : test['class'] })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   actual                 id predicted\n",
      "13      Q   31cedeb4e04fba02         Q\n",
      "20      S   af7dd6b70d544b56         S\n",
      "21      Q   584d5d4428d60a5f         S\n",
      "24      Q   9140ee537fbe5390         Q\n",
      "25      S   cabf9e317ba4a072         S\n",
      "29      Q   3d25a26134f0e450         Q\n",
      "32      S   280b0360e0d3ffc1         S\n",
      "37      Q   0d4a13fc4cce6dab         Q\n",
      "39      C   35179a54ea587953         C\n",
      "40      C   8cdda20f1ae22213         C\n",
      "43      Q   8798ff1fe7ac435d         Q\n",
      "46      S   bc013bdd28614223         S\n",
      "68      Q   7055c710336d670c         Q\n",
      "70      Q   3b416352816dc854         Q\n",
      "73      S   601fdf6ab85a9875         S\n",
      "76      S   498b643ac17bcc7d         C\n",
      "78      S   64e22039495c59bf         S\n",
      "80      S   cc0c263a455bb702         S\n",
      "82      C   8b1a9953c4611296         C\n",
      "85      S   6b2d6039a794fb49         S\n",
      "87      S   94590dd047fcbfce         S\n",
      "91      Q   7a0fc645497df2c6         Q\n",
      "96      S   ecef7fa7fcb25f20         S\n"
     ]
    }
   ],
   "source": [
    "print(predout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds   C  Q   S\n",
      "actual          \n",
      "C       3  0   0\n",
      "Q       0  8   1\n",
      "S       1  0  10\n",
      "\n",
      " preds        C      Q      S\n",
      "actual                      \n",
      "C       100.00   0.00   0.00\n",
      "Q         0.00  88.89  11.11\n",
      "S         9.09   0.00  90.91\n",
      "\n",
      "\n",
      "Accuracy Score:  0.913\n"
     ]
    }
   ],
   "source": [
    "## Cross-check accuracy ##\n",
    "print(pd.crosstab(test['class'], preds, rownames=['actual'], colnames=['preds']))\n",
    "print(\"\\n\",pd.crosstab(test['class'], preds, rownames=['actual']\n",
    "                       , colnames=['preds']).apply(lambda r: round(r/r.sum()*100,2), axis=1) )\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"\\n\\nAccuracy Score: \", round(accuracy_score(test['class'], preds),3) ) # https://en.wikipedia.org/wiki/Jaccard_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flaws in the Approach and Further Validation\n",
    "The accuracy appears pretty good, but the approach taken probably means we have over-fitted the feature selection. In the next section we try out the model on a completely different data-set, taken from the Python FaQ at https://docs.python.org/3/faq/general.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Sentence Data and Generate Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FNAME = 'pythonFAQ.csv'\n",
    "\n",
    "import csv\n",
    "import hashlib\n",
    "import features\n",
    "\n",
    "fin = open(FNAME,'rt')\n",
    "reader = csv.reader(fin)\n",
    "\n",
    "keys = [\"id\",\n",
    "       \"wordCount\",\n",
    "       \"stemmedCount\",\n",
    "       \"stemmedEndNN\",\n",
    "       \"CD\",\n",
    "       \"NN\",\n",
    "       \"NNP\",\n",
    "       \"NNPS\",\n",
    "       \"NNS\",\n",
    "       \"PRP\",\n",
    "       \"VBG\",\n",
    "       \"VBZ\",\n",
    "       \"startTuple0\",\n",
    "       \"endTuple0\",\n",
    "       \"endTuple1\",\n",
    "       \"endTuple2\",\n",
    "       \"verbBeforeNoun\",\n",
    "       \"qMark\",\n",
    "       \"qVerbCombo\",\n",
    "       \"qTripleScore\",\n",
    "       \"sTripleScore\",\n",
    "       \"class\"]\n",
    "\n",
    "rows = []\n",
    "next(reader) #assume we have a header\n",
    "\n",
    "for line in reader:\n",
    "    sentence = line[0]\n",
    "    c = line[1] #class label\n",
    "    id = hashlib.md5(str(sentence).encode('utf-8')).hexdigest()[:16] #generate a unique id \n",
    "    f = features.features_dict(id,sentence,c)\n",
    "    row = []\n",
    "    \n",
    "    for key in keys:\n",
    "        value = f[key]\n",
    "        row.append(value)\n",
    "    rows.append(row)\n",
    "    \n",
    "faq = pd.DataFrame(rows,columns= keys)\n",
    "fin.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the Class of Sentence with Previously Built Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict against FAQ test set \n",
    "featureNames = faq.columns[1:width-1] #remove the first ID col and last col =classifier\n",
    "feqPreds = clf.predict(faq[featureNames])\n",
    "\n",
    "predout = pd.DataFrame({'id':faq['id'],'predicted':feqPreds,'actual':faq['class']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Check Accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds    C   Q   S\n",
      "actual            \n",
      "C       12   5   2\n",
      "Q        0  14   2\n",
      "S        0   3  13\n",
      "\n",
      " preds       C      Q      S\n",
      "actual                     \n",
      "C       63.16  26.32  10.53\n",
      "Q        0.00  87.50  12.50\n",
      "S        0.00  18.75  81.25\n"
     ]
    }
   ],
   "source": [
    "print(pd.crosstab(faq['class'],feqPreds,rownames=['actual'],colnames=['preds']))\n",
    "print(\"\\n\",pd.crosstab(faq['class'], feqPreds, rownames=['actual'],\n",
    "                       colnames=['preds']).apply(lambda r: round(r/r.sum()*100,2), axis=1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.765\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy Score:\", round(accuracy_score(faq['class'], feqPreds) ,3) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This could be summarised as \"OK\" but not great ...\n",
    "\n",
    "The Question and Statement predictions are reported as greater than 80% accurate and the features extraction method could easily be expanded on and enhanced.\n",
    "\n",
    "Also the training data-set is small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Ad-hoc testing and experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " Prediction is: STATEMENT\n"
     ]
    }
   ],
   "source": [
    "textout = {'Q':\"QUESTION\",'C':\"CHAT\",'S':\"STATEMENT\"}\n",
    "\n",
    "my_sentence = \"Scikit learn is a popular Python library for machine learning.\"\n",
    "#mySentence = \"The cat is dead\"\n",
    "#mySentence = \"Is the cat dead\"\n",
    "\n",
    "my_features = features.features_dict('1',my_sentence,'X')\n",
    "\n",
    "values =[]\n",
    "for key in keys:\n",
    "    values.append(my_features[key])\n",
    "\n",
    "s = pd.Series(values)\n",
    "width = len(s)\n",
    "my_features = s[1:width-1] # all but the last item (this is the class for supervised learning mode)\n",
    "predict = clf.predict([my_features])\n",
    "\n",
    "print(\"\\n \\n Prediction is:\",textout[predict[0].strip()])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
